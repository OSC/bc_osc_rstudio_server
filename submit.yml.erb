<%-
  cores = num_cores.to_i

  if cores == 0 && cluster == "pitzer"
    # little optimization for pitzer nodes. They want the whole node, if they chose 'any',
    # it can be scheduled on p18 or p20 nodes. If not, they'll get the constraint below.
    base_slurm_args = ["--nodes", "1", "--exclusive"]
  elsif cores == 0
    # full node on owens
    cores = 28
    base_slurm_args = ["--nodes", "1", "--ntasks-per-node", "28"]
  else
    base_slurm_args = ["--nodes", "1", "--ntasks-per-node", "#{cores}"]
  end

  slurm_args = case node_type
              when "gpu"
                base_slurm_args += ["--gpus-per-node", "1"]
              when "gpu-40core"
                base_slurm_args += ["--gpus-per-node", "1", "--constraint", "40core"]
              when "gpu-48core"
                base_slurm_args += ["--gpus-per-node", "1", "--constraint", "48core"]
              when "any-40core"
                base_slurm_args += ["--constraint", "40core"]
              when "any-48core"
                base_slurm_args += ["--constraint", "48core"]
              when "hugemem"
                base_slurm_args += ["--partition", "hugemem", "--exclusive"]
              when "largemem"
                base_slurm_args += ["--partition", "largemem", "--exclusive"]
              when "debug"
                # partition handled in the script.queue_name below. Need to change when Owens switches to Slurm
                base_slurm_args += ["--exclusive"]
              else
                base_slurm_args
              end

  torque_args = case node_type
              when "gpu"
                ":ppn=#{cores}:gpus=1"
              when "hugemem"
                # disregard what num_cores was submitted (0 or 48 are the only valid options)
                ":ppn=48"
              else
                ":ppn=#{cores}"
              end

  lmod_file = "/users/reporting/lmod/#{cluster}.json"
  lmod_raw = JSON.parse(File.read(lmod_file))
  available_modules = lmod_raw.map do |default_version, modules|
    versions = []
    if modules.is_a?(Hash)
      modules.each do |version_key, version_value|
        versions.concat [ version_value['fullName'] ]
      end
    end
    [ default_version ] + versions
  end.flatten.to_a

  extra_modules.split.each do |selected_module|
    unless available_modules.include?(selected_module)
      raise StandardError, "The #{selected_module} module is not available on the #{cluster} cluster."
    end
  end

-%>
---
batch_connect:
  template: "basic"
script:
  <%- if node_type == "debug" -%>
  queue_name: "debug"
  <%- end -%>
  native:
    <%- if cluster == 'owens' %>
    resources:
      nodes: "1<%= torque_args %>"
    <%- else %>
    <%- slurm_args.each do |arg| %>
    - "<%= arg %>"
    <%- end %>
    <%- end %>