---
cluster: 
  - "owens"
  - "pitzer"
form:
  - version
  - bc_account
  - bc_num_hours
  - node_type
  - num_cores
  - include_tutorials
  - bc_email_on_started
attributes:
  num_cores:
    widget: "number_field"
    label: "Number of cores"
    value: 1
    help: |
      Number of cores on node type (4 GB per core unless requesting whole
      node). Leave blank if requesting full node.
    min: 1
    max: 28
    step: 1
  bc_account:
    label: "Project"
    help: "You can leave this blank if **not** in multiple projects."
  node_type:
    widget: select
    label: "Node type"
    help: |
      - **any** - (*1-28 cores*) Use any available Owens node. This reduces the
        wait time as there are no node requirements.
      - **hugemem** - (*48 cores*) Use an Owens node that has 1.5TB of
        available RAM as well as 48 cores. There are 16 of these nodes on
        Owens. Requesting hugemem nodes allocates entire nodes.
      - **debug** - (*1-28 cores*) For short sessions (= 1 hour) the debug
        queue will have the shortest wait time. This is only accessible during
        8AM - 6PM, Monday - Friday. There are 6 of these nodes on Owens.
      - **gpu** - (*1-28 cores*) Use an Owens node that has an [NVIDIA Tesla P100
        GPU] and loads the appropriate [CUDA] module(s). There are 160 of these nodes
        on Owens. GPU nodes are only available for R version 3.6.3.
      
      [NVIDIA Tesla P100 GPU]: http://www.nvidia.com/object/tesla-p100.html
      [CUDA]: https://developer.nvidia.com/cuda-zone
    options:
      - [
          "any",     "any",
          data-min-ppn: 0,
          data-max-ppn: 28,
          data-cluster: 'owens',
          data-can-show-cuda: false
        ]
      - [
          "gpu",     "gpu",
          data-min-ppn: 0,
          data-max-ppn: 28,
          data-cluster: 'owens',
          data-can-show-cuda: true
        ]
      - [
          "hugemem", "hugemem",
          data-min-ppn: 48,
          data-max-ppn: 48,
          data-cluster: 'owens',
          data-can-show-cuda: false
        ]
      - [
          "debug",   "debug",
          data-min-ppn: 0,
          data-max-ppn: 28,
          data-cluster: 'owens',
          data-can-show-cuda: false
        ]
      - [
          "any",     "any",
          data-min-ppn: 0,
          data-max-ppn: 40,
          data-cluster: 'pitzer',
          data-can-show-cuda: false
        ]
      - [
          "gpu",     "gpu",
          data-min-ppn: 0,
          data-max-ppn: 40,
          data-cluster: 'pitzer',
          data-can-show-cuda: true
        ]
      - [
          "hugemem", "hugemem",
          data-min-ppn: 80,
          data-max-ppn: 80,
          data-cluster: 'pitzer',
          data-can-show-cuda: false
        ]
      - [
          "debug",   "debug",
          data-min-ppn: 0,
          data-max-ppn: 40,
          data-cluster: 'pitzer',
          data-can-show-cuda: false
        ]
  version:
    widget: select
    label: "R version"
    help: "This defines the version of R you want to load."
    options:
      - [ "3.6.3", "app_rstudio_server/3.6.3"]
      - [ "3.6.1", "gnu/9.1.0 mkl/2019.0.3 R/3.6.1 rstudio/1.1.380_server texlive" ]
      - [ "3.6.0", "gnu/7.3.0 mkl/2018.0.3 R/3.6.0 rstudio/1.1.380_server texlive" ]
      - [ "3.5.0", "intel/18.0.3 R/3.5.0 rstudio/1.1.380_server texlive" ]
      - [ "3.4.2", "intel/16.0.3 R/3.4.2 rstudio/1.1.380_server texlive" ]
      - [ "3.3.2", "intel/16.0.3 R/3.3.2 rstudio/1.0.136_server texlive" ]
  include_tutorials:
    widget: "check_box"
    label: Include access to OSC tutorial/workshop materials.
    help: |
      Accessing the tutorial materials will start you with a clean environment.
      While running in tutorial mode you will not have access to the files your $HOME.
